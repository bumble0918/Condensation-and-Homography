{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Canny edge detector to reduce search area - Zuohe Zheng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import io\n",
    "import cv2 as cv \n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import os # for reading all files in a folder\n",
    "pylab.rcParams['figure.figsize'] = (12.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canny Edge detector\n",
    "images = []\n",
    "iFrame = 0\n",
    "folder = 'Pattern01/'\n",
    "\n",
    "for frameNum in sorted(os.listdir(folder)):\n",
    "        images.append(cv.imread(folder+frameNum))\n",
    "        iFrame += 1\n",
    "        \n",
    "imgHeight, imgWidth, colors = images[0].shape\n",
    "edges0 = cv.Canny(images[0],0,100)\n",
    "plt.imshow(edges0)\n",
    "\n",
    "# Use the four edges of the black square to set a boundary\n",
    "up_bound = np.zeros(iFrame)\n",
    "low_bound = np.zeros(iFrame)\n",
    "left_bound = np.zeros(iFrame)\n",
    "right_bound = np.zeros(iFrame)\n",
    "\n",
    "for iframe in range(iFrame):\n",
    "    img = images[iframe]\n",
    "    edges = cv.Canny(img,0,100)\n",
    "    up_y = np.where(edges[0:300,300]>0)\n",
    "    low_y = np.where(edges[301:500,300]>0)\n",
    "    left_x = np.where(edges[300,0:300]>0)\n",
    "    right_x = np.where(edges[300,301:600]>0)\n",
    "    \n",
    "    up_b = np.int(np.max(up_y)-50)\n",
    "    low_b = np.int(np.min(low_y)+300+50)\n",
    "    left_b = np.int(np.max(left_x)-50)\n",
    "    right_b = np.int(np.min(right_x)+300+50)\n",
    "    \n",
    "    # constrain the boundary in the image area\n",
    "    up_bound[iframe] = np.max([up_b,0])\n",
    "    low_bound[iframe] = np.min([low_b,imgHeight])\n",
    "    left_bound[iframe] = np.max([left_b,0])\n",
    "    right_bound[iframe] = np.min([right_b,imgWidth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ur = HW2_Practical9c('ur',2000,15,up_bound,low_bound,left_bound,right_bound)\n",
    "test_ul = HW2_Practical9c('ul',2000,15,up_bound,low_bound,left_bound,right_bound)\n",
    "test_ll = HW2_Practical9c('ll',3000,15,up_bound,low_bound,left_bound,right_bound)\n",
    "test_lr = HW2_Practical9c('lr',2000,15,up_bound,low_bound,left_bound,right_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all images in folder\n",
    "images = []\n",
    "nFrame = 0\n",
    "folder = 'Pattern01/'\n",
    "for frameNum in sorted(os.listdir(folder)):\n",
    "    images.append(cv.imread(folder+frameNum))\n",
    "    nFrame += 1\n",
    "# # plot first image \n",
    "# plt.imshow(images[0])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Coordinates of the known target object (a dark square on a plane) in 3D:\n",
    "XCart = np.array([[-50, -50,  50,  50],\n",
    "          [50, -50, -50,  50],\n",
    "            [0, 0, 0, 0]])\n",
    "\n",
    "# These are some approximate intrinsics for this footage.\n",
    "K = np.array([[640, 0, 320],\n",
    "          [0, 512, 256],\n",
    "            [0, 0, 1]])\n",
    "\n",
    "# Define 3D points of wireframe object.\n",
    "XWireFrameCart = np.array([[-50, -50,  50,  50, -50, -50,  50,  50],\n",
    "          [50, -50, -50,  50, 50, -50, -50,  50],\n",
    "            [0, 0, 0, 0, -100, -100, -100, -100, ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "for iFrame in range(nFrame):\n",
    "    print('Processing Frame', iFrame)\n",
    "    xImCart = np.array([test_ll[iFrame,:].T, test_ul[iFrame,:].T, test_ur[iFrame,:].T, test_lr[iFrame,:].T]).T\n",
    "\n",
    "    # get a frame from footage \n",
    "    im = images[iFrame]\n",
    "\n",
    "    # Draw image and 2d points\n",
    "    plt.imshow(im)\n",
    "    plt.scatter(x = xImCart[0,:], y = xImCart[1,:],c = 'r')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    #TO DO: Use your routine to calculate TEst the extrinsic matrix relating the\n",
    "    #plane position to the camera position.\n",
    "    #T = estimatePlanePose(xImCart, XCart, K);\n",
    "    TEst = estimatePlanePose(xImCart,XCart,K)\n",
    "    \n",
    "    # TO DO: Draw a wire frame cube using data XWireFrameCart. You need to\n",
    "    # 1) project the vertices of a 3D cube through the projective camera;\n",
    "    # 2) draw lines betweeen the resulting 2d image points.\n",
    "    # Note: CONDUCT YOUR CODE FOR DRAWING XWireFrameCart HERE\n",
    "    \n",
    "    XWireFrameCartProjected = projectiveCamera(K,TEst,XWireFrameCart)\n",
    "    plt.plot(XWireFrameCartProjected[0,],XWireFrameCartProjected[1,],'g.')\n",
    "\n",
    "    # Connect the points to draw the frame of the cube\n",
    "    for cPoint in range(4):\n",
    "        plt.plot([XWireFrameCartProjected[0,cPoint],XWireFrameCartProjected[0,cPoint+4]], \\\n",
    "                 [XWireFrameCartProjected[1,cPoint],XWireFrameCartProjected[1,cPoint+4]],'g-') \n",
    "    for cPoint in range(3):\n",
    "        plt.plot([XWireFrameCartProjected[0,cPoint],XWireFrameCartProjected[0,cPoint+1]], \\\n",
    "                 [XWireFrameCartProjected[1,cPoint],XWireFrameCartProjected[1,cPoint+1]],'g-') \n",
    "        plt.plot([XWireFrameCartProjected[0,cPoint+4],XWireFrameCartProjected[0,cPoint+5]], \\\n",
    "                 [XWireFrameCartProjected[1,cPoint+4],XWireFrameCartProjected[1,cPoint+5]],'g-')\n",
    "    plt.plot([XWireFrameCartProjected[0,0],XWireFrameCartProjected[0,3]], \\\n",
    "                 [XWireFrameCartProjected[1,0],XWireFrameCartProjected[1,3]],'g-') \n",
    "    plt.plot([XWireFrameCartProjected[0,4],XWireFrameCartProjected[0,7]], \\\n",
    "                 [XWireFrameCartProjected[1,4],XWireFrameCartProjected[1,7]],'g-') \n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new function of HW2_Practical9c was changed to restrict the position of particles by four boundaries instead of the shape of the image, to reduce the search area. As the seach area shrinks, the noise level can be smaller to concentrate particles. The result shows that now particles can land around only the edges of the black square, which reduce the possibility of error tracking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLikelihood(image, template):\n",
    "    methods = [cv.TM_CCOEFF, cv.TM_CCOEFF_NORMED, cv.TM_CCORR,\n",
    "            cv.TM_CCORR_NORMED, cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    likelihood = cv.matchTemplate(image, template, methods[0])  \n",
    "    pad_first = int(template.shape[0])\n",
    "    pad_second = int(template.shape[1])\n",
    "    pad_amounts = ((0, pad_first-1), (0, pad_second-1))\n",
    "    likelihood = np.pad(likelihood, pad_amounts, 'constant')\n",
    "    likelihood[likelihood<0] = 0 # to avoid negative weights \n",
    "    kernel = np.ones((10,10),np.float32)/100\n",
    "    smoothed = cv.filter2D(likelihood,-1,kernel) \n",
    "    return smoothed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HW2_Practical9c(corner,particlesNum,std,up_bound,low_bound,left_bound,right_bound):\n",
    "    template = sp.io.loadmat(corner+'.mat')['pixelsTemplate']\n",
    "    # let's show the template\n",
    "#     print('We are matching this template with shape: ', template.shape)\n",
    "#     plt.imshow(template)\n",
    "#     plt.show()\n",
    "\n",
    "    # Load all images in folder\n",
    "    images = []\n",
    "    iFrame = 0\n",
    "    folder = 'Pattern01/'\n",
    "    for frameNum in sorted(os.listdir(folder)):\n",
    "        images.append(cv.imread(folder+frameNum))\n",
    "        iFrame += 1\n",
    "#     # plot first image \n",
    "#     plt.imshow(images[0])\n",
    "#     plt.show()\n",
    "\n",
    "    imgHeight, imgWidth, colors = images[0].shape\n",
    "    numParticles = particlesNum;\n",
    "    weight_of_samples = np.ones((numParticles,1))\n",
    "\n",
    "    # TO DO: normalize the weights (may be trivial this time) [done]\n",
    "    weight_of_samples = weight_of_samples / np.sum(weight_of_samples) #replace this \n",
    "\n",
    "    # Initialize which samples from \"last time\" we want to propagate: all of\n",
    "    # them!:\n",
    "    samples_to_propagate = range(0, numParticles)\n",
    "\n",
    "\n",
    "    # ============================\n",
    "    # NOT A TO DO: You don't need to change the code below, but eventually you may\n",
    "    # want to vary the number of Dims (compare for example to lab 9b) \n",
    "    numDims_w = 2;\n",
    "    # Here we randomly initialize some particles throughout the space of w:\n",
    "    particles_old = np.random.rand(numParticles, numDims_w)\n",
    "    particles_old[:,0] = particles_old[:,0] * imgHeight\n",
    "    particles_old[:,1] = particles_old[:,1] * imgWidth\n",
    "    # ============================\n",
    "\n",
    "    #Initialize a temporary array r to store the per-frame MAP estimate of w. This is what we'll return in the end.\n",
    "    r = np.zeros((iFrame, numDims_w));\n",
    "\n",
    "    for iTime in range(iFrame):\n",
    "#         print('Processing Frame', iTime)\n",
    "        # TO DO: compute the cumulative sume of the weights. [done]\n",
    "#         cum_hist_of_weights = np.linspace(0, 1, numParticles) # replace this\n",
    "        cum_hist_of_weights = np.zeros(numParticles)\n",
    "        for c in range(numParticles):\n",
    "            cum_hist_of_weights[c] = np.sum(weight_of_samples[:c+1])\n",
    "        #print(weight_of_samples)\n",
    "\n",
    "\n",
    "        # ==============================================================\n",
    "        # Resample the old distribution at time t-1, and select samples, favoring\n",
    "        # those that had a higher posterior probability.\n",
    "        # ==============================================================\n",
    "        samples_to_propagate = np.zeros(numParticles, dtype=np.int32)\n",
    "\n",
    "        # Pick random thresholds in the cumulative probability's range [0,1]:\n",
    "        some_threshes = np.random.rand(numParticles)\n",
    "\n",
    "\n",
    "        # For each random threshold, find which sample in the ordered set is\n",
    "        # the first one to push the cumulative probability above that\n",
    "        # threshold, e.g. if the cumulative histogram goes from 0.23 to 0.26\n",
    "        # between the 17th and 18th samples in the old distribution, and the\n",
    "        # threshold is 0.234, then we'll want to propagate the 18th sample's w\n",
    "        # (i.e. particle #18).\n",
    "\n",
    "        for sampNum in range(numParticles): \n",
    "            thresh = some_threshes[sampNum]\n",
    "            for index in range (numParticles):\n",
    "                if cum_hist_of_weights[index] > thresh:\n",
    "                    break\n",
    "            samples_to_propagate[sampNum] = index\n",
    "\n",
    "        # Note: it's ok if some of the old particles get picked repeatedly, while\n",
    "        # others don't get picked at all.\n",
    "\n",
    "\n",
    "        # =================================================\n",
    "        # Visualize if you want\n",
    "        # =================================================\n",
    "#         plt.title('Cumulative histogram of probabilities for sorted list of particles')\n",
    "#         plt.plot(np.zeros(numParticles), some_threshes,'b.')\n",
    "#         plt.plot(range(0, numParticles), cum_hist_of_weights, 'rx-')\n",
    "#         which_sample_ids = np.unique(samples_to_propagate)\n",
    "#         how_many_of_each = np.bincount(np.ravel(samples_to_propagate))\n",
    "#         for k in range(len(which_sample_ids)):\n",
    "#            plt.plot(which_sample_ids[k], 0, 'bo-', markersize = 3 * how_many_of_each[k], markerfacecolor='white')\n",
    "#         plt.xlabel('Indeces of all available samples, with larger blue circles for frequently re-sampled particles\\n(Iteration %01d)' % iTime)\n",
    "#         plt.ylabel('Cumulative probability');\n",
    "#         plt.show()\n",
    "        # =================================================\n",
    "        # =================================================\n",
    "\n",
    "        # Predict where the particles we sampled from the old distribution of \n",
    "        # state-space will go in the next time-step. This means we have to apply \n",
    "        # the motion model to each old sample.\n",
    "        particles_new = np.zeros_like(particles_old)\n",
    "        for particleNum in range(numParticles):\n",
    "            # TO DO: Incorporate some noise, e.g. Gaussian noise with std 20,\n",
    "            # into the current location (particles_old), to give a Brownian\n",
    "            # motion model.\n",
    "            particles_new[particleNum, :] = particles_old[samples_to_propagate[particleNum], :] + np.random.normal(0,std,numDims_w)\n",
    "            #replace this\n",
    "            \n",
    "        # TO DO: Not initially, but change the motion model above to have\n",
    "        # different degrees of freedom, and optionally completely different\n",
    "        # motion models. See Extra Credit for more instructions.\n",
    "\n",
    "        #calculate likelihood function\n",
    "        likelihood = computeLikelihood(images[iTime], template)\n",
    "\n",
    "# #         plot results\n",
    "#         f, axarr = plt.subplots(1, 2)\n",
    "#         axarr[0].imshow(images[iTime])\n",
    "#         axarr[0].set_title('Particles')\n",
    "#         # now draw the particles onto the image\n",
    "#         axarr[0].plot(particles_new[:,1]+template.shape[1]/2, particles_new[:,0]+template.shape[0]/2, 'rx')\n",
    "\n",
    "#         #plot the likelihood\n",
    "#         axarr[1].imshow(likelihood)\n",
    "#         axarr[1].set_title('Likelihood')\n",
    "\n",
    "        # From here we incorporate the data for the new state (time t):\n",
    "        # The new particles accompanying predicted locations in state-space\n",
    "        # for time t, are missing their weights: how well does each particle\n",
    "        # explain the observations x_t?\n",
    "        for particleNum in range(numParticles):\n",
    "\n",
    "            # Convert the particle from state-space w to measurement-space x:\n",
    "            # Note: that step is trivial here since both are in 2D space of image\n",
    "            # coordinates\n",
    "\n",
    "            # Within the loop, we evaluate the likelihood of each particle:\n",
    "            particle = particles_new[particleNum, :]\n",
    "            # Check that the predicted location is a place we can really evaluate\n",
    "            # the likelihood.\n",
    "            inFrame = particle[0] >= up_bound[iTime] and  particle[0] <= low_bound[iTime] and particle[1] >= left_bound[iTime] and particle[1] <= right_bound[iTime]            \n",
    "            if inFrame:\n",
    "                minX = particle[1]\n",
    "                minY = particle[0]\n",
    "\n",
    "                weight_of_samples[particleNum] = likelihood[int(minY), int(minX)]\n",
    "\n",
    "            else:\n",
    "                weight_of_samples[particleNum] = 0.0\n",
    "\n",
    "        # TO DO: normalize the weights [done]\n",
    "        weight_of_samples = weight_of_samples / np.sum(weight_of_samples) # replace this\n",
    "        \n",
    "        # find the location of the particle with highest weight\n",
    "        indices = np.argsort(weight_of_samples,axis = 0)\n",
    "        bestScoringParticles = particles_new[np.squeeze(indices[-15:]), :]\n",
    "#         plt.plot(bestScoringParticles[:,1], bestScoringParticles[:,0], 'rx')\n",
    "        # Return the MAP of middle position. Add template.shape/2 because matchTemplate finds the position of the upper left corner \n",
    "        # of the template. We want to plot the centre of the template. \n",
    "        r[iTime,:] = bestScoringParticles[-1,1]+template.shape[1]/2,bestScoringParticles[-1,0]+template.shape[0]/2\n",
    "#         print(r[iTime,:])   \n",
    "#         plt.show()\n",
    "\n",
    "#         #print the original image and the position of the tracked corner.\n",
    "#         plt.imshow(images[iTime])\n",
    "#         plt.plot(r[iTime,0],r[iTime,1],'rx')\n",
    "#         plt.show()\n",
    "        # Now we're done updating the state for time t. \n",
    "        # For Condensation, just clean up and prepare for the next round of \n",
    "        # predictions and measurements:\n",
    "        particles_old = particles_new\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The goal of this function is to project points in XCart through projective camera\n",
    "#defined by intrinsic matrix K and extrinsic matrix T.\n",
    "def projectiveCamera(K,T,XCart):\n",
    "    \n",
    "    # TO DO: Replace this\n",
    "    # XImCart =\n",
    "\n",
    "    # TO DO: Convert Cartesian 3d points XCart to homogeneous coordinates XHom\n",
    "    # by appending a row of 1 to the original point coordinates to form [u,v,w,1]\n",
    "    XHom = np.concatenate((XCart, np.ones((1,XCart.shape[1]))), axis=0)\n",
    "    # TO DO: Apply extrinsic matrix to XHom, to move to frame of reference of camera\n",
    "    # = lambda * [x',y',1, 1/lambda]\n",
    "    xCamHom1 = T @ XHom\n",
    "    # TO DO: Project points into normalized camera coordinates xCamHom (remove 4th row)\n",
    "    # = lambda * [x',y',1]\n",
    "    xCamHom = xCamHom1[0:3,:]\n",
    "    # TO DO: Move points to image coordinates xImHom by applying intrinsic matrix\n",
    "    # = lambda * [x,y,1]\n",
    "    xImHom = K @ xCamHom\n",
    "    # TO DO: Convert points back to Cartesian coordinates xImCart\n",
    "    # x = (lambda * x) / lambda\n",
    "    # y = (lambda * y) / lambda\n",
    "    XImCart = xImHom[0:2,:] / np.tile([xImHom[2,:]],(2,1))\n",
    "    return XImCart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveAXEqualsZero(A):\n",
    "    # TO DO: Write this routine - it should solve Ah = 0   \n",
    "    h = np.zeros(shape = [np.size(A),1])\n",
    "    [U,L,Vt] = np.linalg.svd(A)\n",
    "    V = np.transpose(Vt)\n",
    "    h = V[:,-1]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal of function is to estimate pose of plane relative to camera (extrinsic matrix)\n",
    "# given points in image xImCart, points in world XCart and intrinsic matrix K\n",
    "\n",
    "def estimatePlanePose(XImCart,XCart,K):\n",
    "\n",
    "    # TO DO: replace this\n",
    "    #T = \n",
    "\n",
    "    # TO DO: Convert Cartesian image points XImCart to homogeneous representation XImHom\n",
    "    # by appending a row of 1 to the original point coordinates to form [x,y,1]\n",
    "    XImHom = np.concatenate((XImCart, np.ones((1,XImCart.shape[1]))), axis=0)\n",
    "    \n",
    "    # TO DO: Convert image co-ordinates XImHom to normalized camera coordinates XCamHom    \n",
    "    # multiply with inverse of intrinsic matrix\n",
    "    XCamHom = np.linalg.inv(K) @ XImHom\n",
    "    \n",
    "    # TO DO: Estimate homography H mapping homogeneous (x,y) coordinates of positions\n",
    "    # in real world to XCamHom (convert XCamHom to Cartesian, calculate the homography) -\n",
    "    # use the routine you wrote for Practical 1B\n",
    "    \n",
    "    # Extract the u and v coordinates of Cartesian 3d points XCart\n",
    "    XCart = XCart[0:2,:]\n",
    "    \n",
    "    # Convert XCart to homogeneous coordinates XCartHom\n",
    "    # [u,v] to [u,v,1]\n",
    "    XCartHom = np.concatenate((XCart, np.ones((1,XCart.shape[1]))), axis=0)\n",
    "    \n",
    "    # Then construct the matrix A, size (n_points,9) \n",
    "    n_points = np.shape(XCamHom)[1]\n",
    "    A = np.zeros(shape = [n_points*2,9])\n",
    "    for n in range(n_points):\n",
    "        A[2*n,[0,1,2]] = 0\n",
    "        A[2*n,[3,4,5]] = -XCartHom[:,n]\n",
    "        A[2*n,[6,7,8]] = XCartHom[:,n] * XCamHom[1,n]\n",
    "        A[2*n+1,[0,1,2]] = XCartHom[:,n]\n",
    "        A[2*n+1,[3,4,5]] = 0\n",
    "        A[2*n+1,[6,7,8]] = -XCartHom[:,n] * XCamHom[0,n]\n",
    "    \n",
    "    # Solve Ah = 0\n",
    "    h = solveAXEqualsZero(A)\n",
    "    # Reshape h into the matrix H, values of h go first into rows of H\n",
    "    h_width = np.sqrt(np.size(h))\n",
    "    h_width = int(h_width)\n",
    "    H = np.reshape(h,[h_width,h_width])\n",
    "     \n",
    "    # TO DO: Estimate first two columns of rotation matrix R from the first two\n",
    "    # columns of H using the SVD\n",
    "    # SVD decomposition of the first two columns of H\n",
    "    [U,L,Vt] = np.linalg.svd(H[:,0:2])\n",
    "    R = np.zeros(shape=[h_width,h_width])\n",
    "    # Replace L with [1,0;0,1;0,0] to form the first two columns of R\n",
    "    l = np.array([[1,0],[0,1],[0,0]])\n",
    "    R[:,0:2] = U @ l @ Vt\n",
    "\n",
    "    # TO DO: Estimate the third column of the rotation matrix by taking the cross\n",
    "    # product of the first two columns\n",
    "    R[:,2] = np.cross(R[:,0],R[:,1])\n",
    "        \n",
    "    # TO DO: Check that the determinant of the rotation matrix is positive - if\n",
    "    # not then multiply last column by -1.\n",
    "    if np.linalg.det(R) <= 0 :\n",
    "        R[:,-1] = - R[:,-1]\n",
    "    \n",
    "    # TO DO: Estimate the translation t by finding the appropriate scaling factor k\n",
    "    # and applying it to the third colulmn of H\n",
    "    # Find translation scaling factor between old and new values\n",
    "    Lamb = H / R\n",
    "    lamb = np.sum(Lamb[0:3,0:2]) / 6\n",
    "    t = H[:,-1] / lamb\n",
    "    \n",
    "    # TO DO: Check whether t_z is negative - if it is then multiply t by -1 and\n",
    "    # the first two columns of R by -1.\n",
    "    if t[-1] < 0 :\n",
    "        t = -t\n",
    "        R[:,0:2] = -R[:,0:2]\n",
    "    \n",
    "            \n",
    "    # TO DO: Assemble transformation into matrix form\n",
    "    # append [Tx,Ty,Tz] to R as the last column\n",
    "    # then append [0,0,0,1] as the last row to form the final transformation matrix\n",
    "    t = np.reshape(t,[3,1])\n",
    "    T = np.concatenate((R, t), axis=1)\n",
    "    T = np.concatenate((T, np.array([[0,0,0,1]])), axis=0)\n",
    "    \n",
    "    return T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HW2_Practical9c_show(corner,particlesNum,std,up_bound,low_bound,left_bound,right_bound):\n",
    "    template = sp.io.loadmat(corner+'.mat')['pixelsTemplate']\n",
    "    # let's show the template\n",
    "    print('We are matching this template with shape: ', template.shape)\n",
    "    plt.imshow(template)\n",
    "    plt.show()\n",
    "\n",
    "    # Load all images in folder\n",
    "    images = []\n",
    "    iFrame = 0\n",
    "    folder = 'Pattern01/'\n",
    "    for frameNum in sorted(os.listdir(folder)):\n",
    "        images.append(cv.imread(folder+frameNum))\n",
    "        iFrame += 1\n",
    "    # plot first image \n",
    "    plt.imshow(images[0])\n",
    "    plt.show()\n",
    "\n",
    "    imgHeight, imgWidth, colors = images[0].shape\n",
    "    numParticles = particlesNum;\n",
    "    weight_of_samples = np.ones((numParticles,1))\n",
    "\n",
    "    # TO DO: normalize the weights (may be trivial this time) [done]\n",
    "    weight_of_samples = weight_of_samples / np.sum(weight_of_samples) #replace this \n",
    "\n",
    "    # Initialize which samples from \"last time\" we want to propagate: all of\n",
    "    # them!:\n",
    "    samples_to_propagate = range(0, numParticles)\n",
    "\n",
    "\n",
    "    # ============================\n",
    "    # NOT A TO DO: You don't need to change the code below, but eventually you may\n",
    "    # want to vary the number of Dims (compare for example to lab 9b) \n",
    "    numDims_w = 2;\n",
    "    # Here we randomly initialize some particles throughout the space of w:\n",
    "    particles_old = np.random.rand(numParticles, numDims_w)\n",
    "    particles_old[:,0] = particles_old[:,0] * imgHeight\n",
    "    particles_old[:,1] = particles_old[:,1] * imgWidth\n",
    "    # ============================\n",
    "\n",
    "    #Initialize a temporary array r to store the per-frame MAP estimate of w. This is what we'll return in the end.\n",
    "    r = np.zeros((iFrame, numDims_w));\n",
    "\n",
    "    for iTime in range(0,21):\n",
    "        print('Processing Frame', iTime)\n",
    "        # TO DO: compute the cumulative sume of the weights. [done]\n",
    "#         cum_hist_of_weights = np.linspace(0, 1, numParticles) # replace this\n",
    "        cum_hist_of_weights = np.zeros(numParticles)\n",
    "        for c in range(numParticles):\n",
    "            cum_hist_of_weights[c] = np.sum(weight_of_samples[:c+1])\n",
    "        #print(weight_of_samples)\n",
    "\n",
    "\n",
    "        # ==============================================================\n",
    "        # Resample the old distribution at time t-1, and select samples, favoring\n",
    "        # those that had a higher posterior probability.\n",
    "        # ==============================================================\n",
    "        samples_to_propagate = np.zeros(numParticles, dtype=np.int32)\n",
    "\n",
    "        # Pick random thresholds in the cumulative probability's range [0,1]:\n",
    "        some_threshes = np.random.rand(numParticles)\n",
    "\n",
    "\n",
    "        # For each random threshold, find which sample in the ordered set is\n",
    "        # the first one to push the cumulative probability above that\n",
    "        # threshold, e.g. if the cumulative histogram goes from 0.23 to 0.26\n",
    "        # between the 17th and 18th samples in the old distribution, and the\n",
    "        # threshold is 0.234, then we'll want to propagate the 18th sample's w\n",
    "        # (i.e. particle #18).\n",
    "\n",
    "        for sampNum in range(numParticles): \n",
    "            thresh = some_threshes[sampNum]\n",
    "            for index in range (numParticles):\n",
    "                if cum_hist_of_weights[index] > thresh:\n",
    "                    break\n",
    "            samples_to_propagate[sampNum] = index\n",
    "\n",
    "        # Note: it's ok if some of the old particles get picked repeatedly, while\n",
    "        # others don't get picked at all.\n",
    "\n",
    "\n",
    "        # =================================================\n",
    "        # Visualize if you want\n",
    "        # =================================================\n",
    "#         plt.title('Cumulative histogram of probabilities for sorted list of particles')\n",
    "#         plt.plot(np.zeros(numParticles), some_threshes,'b.')\n",
    "#         plt.plot(range(0, numParticles), cum_hist_of_weights, 'rx-')\n",
    "#         which_sample_ids = np.unique(samples_to_propagate)\n",
    "#         how_many_of_each = np.bincount(np.ravel(samples_to_propagate))\n",
    "#         for k in range(len(which_sample_ids)):\n",
    "#            plt.plot(which_sample_ids[k], 0, 'bo-', markersize = 3 * how_many_of_each[k], markerfacecolor='white')\n",
    "#         plt.xlabel('Indeces of all available samples, with larger blue circles for frequently re-sampled particles\\n(Iteration %01d)' % iTime)\n",
    "#         plt.ylabel('Cumulative probability');\n",
    "#         plt.show()\n",
    "        # =================================================\n",
    "        # =================================================\n",
    "\n",
    "        # Predict where the particles we sampled from the old distribution of \n",
    "        # state-space will go in the next time-step. This means we have to apply \n",
    "        # the motion model to each old sample.\n",
    "        particles_new = np.zeros_like(particles_old)\n",
    "        for particleNum in range(numParticles):\n",
    "            # TO DO: Incorporate some noise, e.g. Gaussian noise with std 20,\n",
    "            # into the current location (particles_old), to give a Brownian\n",
    "            # motion model.\n",
    "            particles_new[particleNum, :] = particles_old[samples_to_propagate[particleNum], :] + np.random.normal(0,std,numDims_w)\n",
    "            #replace this\n",
    "            \n",
    "        # TO DO: Not initially, but change the motion model above to have\n",
    "        # different degrees of freedom, and optionally completely different\n",
    "        # motion models. See Extra Credit for more instructions.\n",
    "\n",
    "        #calculate likelihood function\n",
    "        likelihood = computeLikelihood(images[iTime], template)\n",
    "\n",
    "        # plot results\n",
    "        f, axarr = plt.subplots(1, 2)\n",
    "        axarr[0].imshow(images[iTime])\n",
    "        axarr[0].set_title('Particles')\n",
    "        # now draw the particles onto the image\n",
    "        axarr[0].plot(particles_new[:,1]+template.shape[1]/2, particles_new[:,0]+template.shape[0]/2, 'rx')\n",
    "\n",
    "        #plot the likelihood\n",
    "        axarr[1].imshow(likelihood)\n",
    "        axarr[1].set_title('Likelihood')\n",
    "\n",
    "        # From here we incorporate the data for the new state (time t):\n",
    "        # The new particles accompanying predicted locations in state-space\n",
    "        # for time t, are missing their weights: how well does each particle\n",
    "        # explain the observations x_t?\n",
    "        for particleNum in range(numParticles):\n",
    "\n",
    "            # Convert the particle from state-space w to measurement-space x:\n",
    "            # Note: that step is trivial here since both are in 2D space of image\n",
    "            # coordinates\n",
    "\n",
    "            # Within the loop, we evaluate the likelihood of each particle:\n",
    "            particle = particles_new[particleNum, :]\n",
    "            # Check that the predicted location is a place we can really evaluate\n",
    "            # the likelihood.\n",
    "            inFrame = particle[0] >= up_bound[iTime] and  particle[0] <= low_bound[iTime] and particle[1] >= left_bound[iTime] and particle[1] <= right_bound[iTime]            \n",
    "            if inFrame:\n",
    "                minX = particle[1]\n",
    "                minY = particle[0]\n",
    "\n",
    "                weight_of_samples[particleNum] = likelihood[int(minY), int(minX)]\n",
    "\n",
    "            else:\n",
    "                weight_of_samples[particleNum] = 0.0\n",
    "\n",
    "        # TO DO: normalize the weights [done]\n",
    "        weight_of_samples = weight_of_samples / np.sum(weight_of_samples) # replace this\n",
    "        \n",
    "        # find the location of the particle with highest weight\n",
    "        indices = np.argsort(weight_of_samples,axis = 0)\n",
    "        bestScoringParticles = particles_new[np.squeeze(indices[-15:]), :]\n",
    "        plt.plot(bestScoringParticles[:,1], bestScoringParticles[:,0], 'rx')\n",
    "        # Return the MAP of middle position. Add template.shape/2 because matchTemplate finds the position of the upper left corner \n",
    "        # of the template. We want to plot the centre of the template. \n",
    "        r[iTime,:] = bestScoringParticles[-1,1]+template.shape[1]/2,bestScoringParticles[-1,0]+template.shape[0]/2\n",
    "        print(r[iTime,:])   \n",
    "        plt.show()\n",
    "\n",
    "        #print the original image and the position of the tracked corner.\n",
    "        plt.imshow(images[iTime])\n",
    "        plt.plot(r[iTime,0],r[iTime,1],'rx')\n",
    "        plt.show()\n",
    "        # Now we're done updating the state for time t. \n",
    "        # For Condensation, just clean up and prepare for the next round of \n",
    "        # predictions and measurements:\n",
    "        particles_old = particles_new\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
